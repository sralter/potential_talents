{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b2f32a-c997-4d43-968a-a0963e556dbc",
   "metadata": {},
   "source": [
    "# Potential Talents - An Apziva Project (#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45280031-db40-495c-9dad-cc58a57a5dd7",
   "metadata": {},
   "source": [
    "# Proceed to the [previous notebook](potential_talents_p1.ipynb) to view the EDA and initial NLP work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e075c-97b1-4aa3-8396-fe660e6d9707",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb875fcb-ee22-4a1a-8d4a-8eea9ce24dcf",
   "metadata": {},
   "source": [
    "By Samuel Alter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562e442-1f9d-4e22-977b-94499c9788b8",
   "metadata": {},
   "source": [
    "Apziva: 6bImatZVlK6DnbEo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9720bb-32b3-4f1f-9bc7-dd279f452251",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8ba0a-baad-4823-8240-c128467f8021",
   "metadata": {},
   "source": [
    "We are working with a talent sourcing and management company to help them surface candidates that are a best fit for their human resources job post. We are using a dataset of job candidates' job titles, their location, and their number of LinkedIn connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca00f2c-2194-42b0-90bf-5b650826d022",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7fc0a-a325-4fa0-b3ae-cb6da92530b4",
   "metadata": {},
   "source": [
    "Produce a probability, between 0 and 1, of how closely the candidate fits the job description of **\"Aspiring human resources\"** or **\"Seeking human resources.\"** After an initial recommendation pulls out a candidate(s) to be starred for future consideration, the recommendation will be re-run and new \"stars\" will be awarded.\n",
    "\n",
    "To help predict how the candidates fit, we are tracking the performance of two success metrics:\n",
    "* Rank candidates based on a fitness score\n",
    "* Re-rank candidates when a candidate is starred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4981bb-5dbb-4de7-8f49-e69346c76a05",
   "metadata": {},
   "source": [
    "We also need to do the following:\n",
    "* Explain how the algorithm works and how the ranking improves after each starring iteration\n",
    "* How to filter out candidates which should not be considered at all\n",
    "* Determine a cut-off point (if possible) that would work for other roles without losing high-potential candidates\n",
    "* Ideas to explore on automating this procedure to reduce or eliminate human bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b95e26-1ada-4283-b732-deb0779168d0",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c08ff1-a851-46f8-a66d-192d15d34a91",
   "metadata": {},
   "source": [
    "| Column | Data Type | Comments |\n",
    "|---|---|---|\n",
    "| `id` | Numeric | Unique identifier for the candidate |\n",
    "| `job_title` | Text | Job title for the candidate |\n",
    "| `location` | Text | Geographic location of the candidate |\n",
    "| `connections` | Text | Number of LinkedIn connections for the candidate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264aa5d-7682-4e2f-84cd-b773e5e6f4f9",
   "metadata": {},
   "source": [
    "Connections over 500 are encoded as \"500+\". Some do not have specific locations listed and just had their country, so I substituted capitol cities or geographic centers to represent those countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08a44b-95af-489f-ae1b-94ccffc89d82",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2ae21e-9eb4-46d3-bcca-37176ff47842",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Anaconda/anaconda3/envs/apziva/lib/python3.9/site-packages/pandas/__init__.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/Applications/Anaconda/anaconda3/envs/apziva/lib/python3.9/site-packages/pandas/compat/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     pa_version_under10p1,\n\u001b[1;32m     29\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     30\u001b[0m     pa_version_under13p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under14p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under14p1,\n\u001b[1;32m     33\u001b[0m     pa_version_under16p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[0;32m/Applications/Anaconda/anaconda3/envs/apziva/lib/python3.9/site-packages/pandas/compat/pyarrow.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[1;32m     11\u001b[0m     pa_version_under10p1 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     pa_version_under11p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "from torchview import draw_graph\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ffa7a-553e-4c3a-abd7-418a4830a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to generate random integers\n",
    "\n",
    "def rand_gen(low=1,high=1e4):\n",
    "    '''\n",
    "    Generates a pseudo-random integer\n",
    "    consisting of up to four digits\n",
    "    '''\n",
    "    import numpy as np\n",
    "    rng=np.random.default_rng()\n",
    "    random_state=int(rng.integers(low=low,high=high))\n",
    "    \n",
    "    return random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533b28b-2ea8-410e-a3ce-87b95ac97057",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=rand_gen()\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc5d5b-5495-47b9-9982-84b81f5ba498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the randomness seed throughout the notebook\n",
    "# source: # https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
    "\n",
    "## set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "## set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed)\n",
    "## set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac1dcb-2c3a-49a5-bf1f-195b48feb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [name for name, val in callers_local_vars if val is var]\n",
    "\n",
    "def fileDaterSaver(location: str,\n",
    "                   filetype: str,\n",
    "                   object_,\n",
    "                   extra: str = '',\n",
    "                   verbose: bool = True):\n",
    "\n",
    "    '''\n",
    "    Function that gets a timestamped filename and saves it\n",
    "    to a user-specified location.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    location: str - The location where the file will be saved.\n",
    "    filetype: str - The type of the file to save ('csv' or 'json').\n",
    "    object_: The object to be saved. Should be a pandas DataFrame\n",
    "        for 'csv' or serializable for 'json'.\n",
    "    extra: str - Additional string to include in the filename.\n",
    "    verbose: bool - Whether to print verbose messages.\n",
    "    '''\n",
    "\n",
    "    # get current date and time\n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # print current date and time to check\n",
    "    if verbose:\n",
    "        print('current_datetime:', current_datetime)\n",
    "\n",
    "    # format the datetime for a filename\n",
    "    datetime_suffix = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # create filename with the datetime suffix\n",
    "    if extra != '':\n",
    "        file_name = f'{location}{extra}_{datetime_suffix}.{filetype}'\n",
    "    else:\n",
    "        file_name = f'{location}{datetime_suffix}.{filetype}'\n",
    "\n",
    "    # print file name\n",
    "    if verbose:\n",
    "        print(file_name)\n",
    "\n",
    "    # save object\n",
    "    if filetype == 'csv':\n",
    "        object_.to_csv(file_name, index=True)\n",
    "    elif filetype == 'json':\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(json.dumps(object_, default=str))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use 'csv' or 'json'.\")\n",
    "\n",
    "    # confirm save\n",
    "    file_path = Path(file_name)\n",
    "    if file_path.exists():\n",
    "        variable_name = get_variable_name(object_)\n",
    "        if variable_name:\n",
    "            print(f'Successfully saved {variable_name[0]} to {file_path}')\n",
    "        else:\n",
    "            print(f'Successfully saved object to {file_path}')\n",
    "    else:\n",
    "        print(\"File save error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfe2d7-88dc-43e6-b60c-3c32bbe617fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a77be-eeb8-4d01-a09a-5abea130de44",
   "metadata": {},
   "source": [
    "taken from here: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c27fa-cc4c-462d-a96c-e18e8662117f",
   "metadata": {},
   "source": [
    "Quickstart\n",
    "\n",
    "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
    "\n",
    "Working with data\n",
    "\n",
    "PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf095-00b2-4827-b413-3a4d4881883d",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "```\n",
    "\n",
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
    "\n",
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (full list here). In this tutorial, we use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1f4e0-5b63-4de7-90ae-78a3fe86072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb1021-89fc-476c-b455-42707b32677a",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e43079-4c43-4a2a-8ae8-244db2cb66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdb131-58e8-413c-a124-6abbaa3dba11",
   "metadata": {},
   "source": [
    "Creating Models\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f37604-ae5e-48b4-aaca-d2009906b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb3dfa-1b64-47b8-8442-b50650dd3baf",
   "metadata": {},
   "source": [
    "Optimizing the Model Parameters\n",
    "\n",
    "To train a model, we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e939d-2e94-44e6-a4d0-56575e3ed089",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8dce6-3f43-47e6-a668-91cc9544b345",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326e8b7-0bbc-44b5-be67-5de47f65f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a182d-c8be-4335-8f58-601613283166",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46a4e2-b869-4e9e-8f5c-3787d1b615ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705bab30-e0af-4ed9-b89c-c8d2e776dc0b",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19045d2-0ebc-43f5-9027-d6b064cf9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386de5e-e2c1-4cfc-8b9e-1f969594d704",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130813a-ddd2-4140-a8b2-7df3015f725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path(\"../joblib/3_testmodel.pth\")\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f\"Saved PyTorch Model State to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18c3c7-6fd7-4d55-9be9-452448a2c666",
   "metadata": {},
   "source": [
    "Loading Models\n",
    "\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a867248-c3b4-44e0-a37c-d31dd3dfcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(path, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77540c06-de30-438e-ba89-148ed0ccac30",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45d710-2aa2-4f54-8cb7-49eb435479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86207be9-a635-4388-9c1f-39b6397353a2",
   "metadata": {},
   "source": [
    "Tensor tutorial here: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adae6b-ed93-4dc7-92f9-8bf7b0bd3505",
   "metadata": {},
   "source": [
    "Datasets & dataloaders here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c18f97-0578-4797-9232-7a1793373afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a22802-ca0d-4ea7-b1b9-ee7a109e900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936a7d5-3f90-43bb-968c-82c3667a4678",
   "metadata": {},
   "source": [
    "Optimizing model parameters\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f91978-c307-4c09-8fe5-1cc39f41e9ae",
   "metadata": {},
   "source": [
    "Here is a tutorial about using RankNet in PyTorch that I am also following, especially for help on constructing the neural network:\n",
    "\n",
    "https://medium.com/@mandeep0405/learning-to-rank-ranknet-simplified-5d7f7334133d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1d9ef-d623-40af-b630-35a3294013a7",
   "metadata": {},
   "source": [
    "# Learning to Rank (LTR) systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005e804-69e0-49ab-9439-11b3a438dc1b",
   "metadata": {},
   "source": [
    "[RankNet](https://icml.cc/Conferences/2015/wp-content/uploads/2015/06/icml_ranking.pdf) is an influential algorithm that is designed to rank items given an objective. LTR systems are used for information retrieval, which is crucial for search engines, recommender systems like on Netflix or Amazon, travel agencies, and online advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce01b4-1113-4da2-a7a3-5e4f098ab46b",
   "metadata": {},
   "source": [
    "You can learn more about LTR systems from [this](https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4) Medium post by [Francesco Casalegno](https://medium.com/@francesco.casalegno)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0045d-3218-490c-bf4a-192ca893b4a3",
   "metadata": {},
   "source": [
    "RankNet uses a \"pairwise\" method of training, meaning it predices which item in a pair should be ranked higher based on a probabilistic model. With pairs of items, using RankNet makes our task a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90894799-a03f-40b1-8eba-7eb65ed0994d",
   "metadata": {},
   "source": [
    "Example PyTorch setup with RankNet from [here](https://medium.com/@mandeep0405/learning-to-rank-ranknet-simplified-5d7f7334133d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cecece-0dac-4fd6-ba00-7be738a44e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RankNet\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Process first item\n",
    "        x1_in = self.activation(self.input(x1))\n",
    "        h1 = self.activation(self.hidden(x1_in))\n",
    "        out1 = self.output(h1)\n",
    "\n",
    "        # Process second item\n",
    "        x2_in = self.activation(self.input(x2))\n",
    "        h2 = self.activation(self.hidden(x2_in))\n",
    "        out2 = self.output(h2)\n",
    "\n",
    "        # Compute difference\n",
    "        return out1 - out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9654120-e4cc-4f2b-b992-8ee03e18f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = 10  # dimension of item feature vector e.g., relevance_to_genre, rating etc.\n",
    "hidden_size = 16 # hidden layer params\n",
    "learning_rate = 0.01\n",
    "\n",
    "# instantiate model\n",
    "model = RankNet(input_size, hidden_size)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # applies sigmoid\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd25b32-09d6-453e-994d-214037c2ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "x1 = torch.randn(1, input_size)  # Feature vector for item 1\n",
    "x2 = torch.randn(1, input_size)  # Feature vector for item 2\n",
    "target = torch.tensor([[1.0]])  # 1 if x1 should be ranked higher, 0 otherwise\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    \n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    diff = model(x1, x2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(diff, target)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss.append(loss.item())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e355848-2130-481b-8b4a-137375396629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Loss\n",
    "plt.plot(train_loss, label='train_loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "# plot model architecture\n",
    "model_graph = draw_graph(model, input_data = [x1,x2], graph_dir = \"LR\", save_graph = True, graph_name=\"RankNet\")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab761f72-a093-45b7-9c56-253b7b4a7f9b",
   "metadata": {},
   "source": [
    "from here: https://github.com/yanshanjing/RankNet-Pytorch/blob/master/RankNet-Pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b3ceb-ac1c-406c-a0a4-98fe625c07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9c006-ca1d-40e4-9dcd-501f9d1f4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device for Apple Silicon or NVIDIA\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, num_feature):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear( num_feature, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.output_sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1,input_2):\n",
    "        input_1, input_2 = input_1.to(device), input_2.to(device)\n",
    "        s1 = self.model(input_1)\n",
    "        s2 = self.model(input_2)\n",
    "        out = self.output_sig(s1 - s2)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, input_):\n",
    "        s = self.model(input_)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04d193-2b29-4245-b27f-3b37077ab759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "n_sample = 30000\n",
    "n_feature = 300\n",
    "data1 = torch.rand((n_sample,n_feature)).to(device)\n",
    "data2 = torch.rand((n_sample, n_feature)).to(device)\n",
    "y = torch.rand((n_sample, 1)).to(device)\n",
    "\n",
    "# model, organizer, and loss function setup\n",
    "rank_model = RankNet(num_feature=n_feature).to(device)\n",
    "optimizer = torch.optim.Adam(rank_model.parameters())\n",
    "loss_fun = torch.nn.BCELoss().to(device)\n",
    "\n",
    "# training loop initialization\n",
    "epoch = 1000\n",
    "losses = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    rank_model.train()\n",
    "    rank_model.zero_grad()\n",
    "    y_pred = rank_model(data1, data2)\n",
    "    loss = loss_fun(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 250 == 0:\n",
    "        print(f'Epoch {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad688bf4-78e4-4691-b09f-a23e12b91f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fa546-ff20-485c-b1f2-b217d5a6efa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796856d8-2023-4d85-95d8-952d86422fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30108dc0-7881-40ad-98f0-d1cffcafbba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952f075-91fd-4feb-8d04-2cd90255b5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b85713-acf1-413e-bb29-5a97d20d06f2",
   "metadata": {},
   "source": [
    "## RankNet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb57eb-3856-4ad5-9f90-8e48ef9f38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefe88f-9c90-4508-b055-ec5c6d2f8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_feature, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.output_sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_1, input_2):\n",
    "        input_1, input_2 = input_1.to(device), input_2.to(device)\n",
    "        s1 = self.model(input_1)\n",
    "        s2 = self.model(input_2)\n",
    "        out = self.output_sig(s1 - s2)\n",
    "        return out\n",
    "\n",
    "# these functions are not bound to the class instance    \n",
    "def train(dataloader, model, loss_fn, optimizer, data_points=100):\n",
    "    size = len(dataloader.dataset)\n",
    "    interval = max(1, size // data_points)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % interval == 0:\n",
    "            loss = loss.item()\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf416665-82e8-470b-bcd0-380b89b05432",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 2\n",
    "\n",
    "# instantiate model, optimizer, and loss function\n",
    "model = RankNet(n_feature=n_feature).to(device)\n",
    "optimizer = torch.optim.Adam(rank_model.parameters())\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "pred = model(input_1, input_2)\n",
    "loss = loss_fn(pred, target)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067adbf-ba18-4bf1-98df-9469f1e029b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets for modeling\n",
    "# training_data = \n",
    "# test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10075e-9f96-4869-9f20-d0d81c34fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup batch processing and data loader\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65297890-48e0-454e-896c-20dc90c2859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = 10  # dimension of item feature vector e.g., relevance_to_genre, rating etc.\n",
    "hidden_size = 16 # hidden layer params\n",
    "learning_rate = 0.01\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # applies sigmoid\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apziva",
   "language": "python",
   "name": "apziva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
